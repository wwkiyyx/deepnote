{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-48cf62ec-7888-4fea-8db5-55c0e4af2272",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5218657",
    "execution_start": 1626941203985,
    "execution_millis": 1332692,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1691
   },
   "source": "import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nmnist = datasets.ImageFolder('/work/fashion_mnist', transform=transforms.Compose([\n                    transforms.ToTensor(), transforms.Grayscale(1), transforms.Normalize((0.1307,), (0.3081,))]))\ntrain_dataset, test_dataset = torch.utils.data.random_split(mnist, [60000, 10000])\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\nmodel = NeuralNetwork()\nprint(model)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_size = len(train_dataloader.dataset)\n    for batch, (X, y) in enumerate(train_dataloader):\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if batch % 1000 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n    test_size = len(test_dataloader.dataset)\n    num_batches = len(test_dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in test_dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= test_size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\nprint(\"Done!\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n    (5): ReLU()\n  )\n)\nEpoch 1\n-------------------------------\nloss: 2.255802  [    0/60000]\nloss: 0.521048  [ 4000/60000]\nloss: 1.096964  [ 8000/60000]\nloss: 0.189301  [12000/60000]\nloss: 0.843151  [16000/60000]\nloss: 0.445399  [20000/60000]\nloss: 0.261730  [24000/60000]\nloss: 0.130176  [28000/60000]\nloss: 0.399708  [32000/60000]\nloss: 0.107442  [36000/60000]\nloss: 0.281995  [40000/60000]\nloss: 0.195686  [44000/60000]\nloss: 0.053593  [48000/60000]\nloss: 1.319013  [52000/60000]\nloss: 1.232422  [56000/60000]\nTest Error: \n Accuracy: 85.9%, Avg loss: 0.377992 \n\nEpoch 2\n-------------------------------\nloss: 0.196188  [    0/60000]\nloss: 0.028772  [ 4000/60000]\nloss: 1.139685  [ 8000/60000]\nloss: 0.286814  [12000/60000]\nloss: 0.512010  [16000/60000]\nloss: 0.008694  [20000/60000]\nloss: 0.012042  [24000/60000]\nloss: 0.880502  [28000/60000]\nloss: 0.184963  [32000/60000]\nloss: 1.369683  [36000/60000]\nloss: 0.231702  [40000/60000]\nloss: 0.007911  [44000/60000]\nloss: 0.138785  [48000/60000]\nloss: 0.156205  [52000/60000]\nloss: 0.664781  [56000/60000]\nTest Error: \n Accuracy: 86.2%, Avg loss: 0.370935 \n\nEpoch 3\n-------------------------------\nloss: 0.135938  [    0/60000]\nloss: 0.047356  [ 4000/60000]\nloss: 0.086018  [ 8000/60000]\nloss: 0.181760  [12000/60000]\nloss: 0.034174  [16000/60000]\nloss: 0.135813  [20000/60000]\nloss: 0.043905  [24000/60000]\nloss: 0.169133  [28000/60000]\nloss: 0.082044  [32000/60000]\nloss: 0.056711  [36000/60000]\nloss: 0.079485  [40000/60000]\nloss: 0.666066  [44000/60000]\nloss: 0.156794  [48000/60000]\nloss: 0.154017  [52000/60000]\nloss: 0.659207  [56000/60000]\nTest Error: \n Accuracy: 88.1%, Avg loss: 0.334445 \n\nEpoch 4\n-------------------------------\nloss: 0.006634  [    0/60000]\nloss: 0.005516  [ 4000/60000]\nloss: 0.036619  [ 8000/60000]\nloss: 0.049378  [12000/60000]\nloss: 0.142365  [16000/60000]\nloss: 0.882962  [20000/60000]\nloss: 0.020716  [24000/60000]\nloss: 0.038694  [28000/60000]\nloss: 0.228729  [32000/60000]\nloss: 0.023000  [36000/60000]\nloss: 0.159900  [40000/60000]\nloss: 0.075658  [44000/60000]\nloss: 0.046493  [48000/60000]\nloss: 0.366000  [52000/60000]\nloss: 0.008728  [56000/60000]\nTest Error: \n Accuracy: 88.2%, Avg loss: 0.326733 \n\nEpoch 5\n-------------------------------\nloss: 0.268963  [    0/60000]\nloss: 0.016367  [ 4000/60000]\nloss: 0.556236  [ 8000/60000]\nloss: 0.094464  [12000/60000]\nloss: 0.000524  [16000/60000]\nloss: 0.602955  [20000/60000]\nloss: 0.208921  [24000/60000]\nloss: 0.001058  [28000/60000]\nloss: 0.128186  [32000/60000]\nloss: 0.260865  [36000/60000]\nloss: 0.820634  [40000/60000]\nloss: 1.181491  [44000/60000]\nloss: 0.200793  [48000/60000]\nloss: 0.492010  [52000/60000]\nloss: 1.179468  [56000/60000]\nTest Error: \n Accuracy: 88.3%, Avg loss: 0.331028 \n\nDone!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-eb024543-fd0c-49f9-908d-cd163f9383b2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a5ef7560",
    "execution_start": 1626945448929,
    "execution_millis": 1313213,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1745
   },
   "source": "import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nmnist = datasets.ImageFolder('/work/fashion_mnist', transform=transforms.Compose([\n                    transforms.ToTensor(), transforms.Grayscale(1), transforms.Normalize((0.1307,), (0.3081,))]))\ntrain_dataset, test_dataset = torch.utils.data.random_split(mnist, [60000, 10000])\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 5)\n        self.fc1   = nn.Linear(1024, 128)\n        #self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(128, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        #out = F.relu(self.fc2(out))\n        out = F.log_softmax(self.fc3(out))\n        return out\nmodel = NeuralNetwork()\nprint(model)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_size = len(train_dataloader.dataset)\n    for batch, (X, y) in enumerate(train_dataloader):\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if batch % 1000 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n    test_size = len(test_dataloader.dataset)\n    num_batches = len(test_dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in test_dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= test_size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\nprint(\"Done!\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "NeuralNetwork(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n)\nEpoch 1\n-------------------------------\n/shared-libs/python3.8/py/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n/tmp/ipykernel_135/3696099549.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  out = F.log_softmax(self.fc3(out))\nloss: 2.354117  [    0/60000]\nloss: 1.050640  [ 4000/60000]\nloss: 0.452324  [ 8000/60000]\nloss: 0.535374  [12000/60000]\nloss: 0.864775  [16000/60000]\nloss: 1.249792  [20000/60000]\nloss: 0.561825  [24000/60000]\nloss: 0.274726  [28000/60000]\nloss: 0.699645  [32000/60000]\nloss: 0.367627  [36000/60000]\nloss: 0.588406  [40000/60000]\nloss: 0.555771  [44000/60000]\nloss: 0.993701  [48000/60000]\nloss: 0.019619  [52000/60000]\nloss: 0.084346  [56000/60000]\nTest Error: \n Accuracy: 87.9%, Avg loss: 0.343683 \n\nEpoch 2\n-------------------------------\nloss: 0.129648  [    0/60000]\nloss: 0.070013  [ 4000/60000]\nloss: 0.064571  [ 8000/60000]\nloss: 0.186184  [12000/60000]\nloss: 0.001418  [16000/60000]\nloss: 0.019493  [20000/60000]\nloss: 0.090607  [24000/60000]\nloss: 0.232025  [28000/60000]\nloss: 1.283801  [32000/60000]\nloss: 0.010380  [36000/60000]\nloss: 0.069413  [40000/60000]\nloss: 0.008320  [44000/60000]\nloss: 0.150615  [48000/60000]\nloss: 0.143676  [52000/60000]\nloss: 0.124207  [56000/60000]\nTest Error: \n Accuracy: 88.6%, Avg loss: 0.312935 \n\nEpoch 3\n-------------------------------\nloss: 0.348948  [    0/60000]\nloss: 0.011925  [ 4000/60000]\nloss: 0.164416  [ 8000/60000]\nloss: 0.387125  [12000/60000]\nloss: 0.229975  [16000/60000]\nloss: 0.310223  [20000/60000]\nloss: 0.811998  [24000/60000]\nloss: 0.061201  [28000/60000]\nloss: 0.003301  [32000/60000]\nloss: 0.152273  [36000/60000]\nloss: 0.250746  [40000/60000]\nloss: 0.137693  [44000/60000]\nloss: 1.124338  [48000/60000]\nloss: 0.006767  [52000/60000]\nloss: 0.810745  [56000/60000]\nTest Error: \n Accuracy: 89.4%, Avg loss: 0.290429 \n\nEpoch 4\n-------------------------------\nloss: 0.346392  [    0/60000]\nloss: 2.251429  [ 4000/60000]\nloss: 0.001075  [ 8000/60000]\nloss: 0.057619  [12000/60000]\nloss: 0.050450  [16000/60000]\nloss: 0.002813  [20000/60000]\nloss: 0.529160  [24000/60000]\nloss: 0.521629  [28000/60000]\nloss: 0.086150  [32000/60000]\nloss: 0.320989  [36000/60000]\nloss: 1.169055  [40000/60000]\nloss: 0.004029  [44000/60000]\nloss: 0.621717  [48000/60000]\nloss: 0.055604  [52000/60000]\nloss: 0.990080  [56000/60000]\nTest Error: \n Accuracy: 90.1%, Avg loss: 0.270591 \n\nEpoch 5\n-------------------------------\nloss: 0.000494  [    0/60000]\nloss: 0.010203  [ 4000/60000]\nloss: 0.201229  [ 8000/60000]\nloss: 0.105359  [12000/60000]\nloss: 0.034147  [16000/60000]\nloss: 0.401592  [20000/60000]\nloss: 0.625009  [24000/60000]\nloss: 0.177832  [28000/60000]\nloss: 0.218607  [32000/60000]\nloss: 0.008310  [36000/60000]\nloss: 0.004823  [40000/60000]\nloss: 0.017478  [44000/60000]\nloss: 0.088006  [48000/60000]\nloss: 0.005670  [52000/60000]\nloss: 0.035531  [56000/60000]\nTest Error: \n Accuracy: 90.6%, Avg loss: 0.261134 \n\nDone!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=26e5d5b4-a604-4f7b-aef4-7756f8127ef0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "367e0708-276e-4642-b0e1-6a68127f3b79",
  "deepnote_execution_queue": []
 }
}