{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-7a11e71f-61fa-41ef-8d16-d341d248f66a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a01a010e",
    "execution_start": 1647308995598,
    "execution_millis": 697924,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1691
   },
   "source": "import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nmnist = datasets.ImageFolder('/work/mnist', transform=transforms.Compose([\n                    transforms.ToTensor(), transforms.Grayscale(1), transforms.Normalize((0.1307,), (0.3081,))]))\ntrain_dataset, test_dataset = torch.utils.data.random_split(mnist, [60000, 10000])\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\nmodel = NeuralNetwork()\nprint(model)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_size = len(train_dataloader.dataset)\n    for batch, (X, y) in enumerate(train_dataloader):\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if batch % 1000 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n    test_size = len(test_dataloader.dataset)\n    num_batches = len(test_dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in test_dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= test_size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\nprint(\"Done!\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.8/py/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n    (5): ReLU()\n  )\n)\nEpoch 1\n-------------------------------\nloss: 2.273217  [    0/60000]\nloss: 2.370657  [ 4000/60000]\nloss: 0.123909  [ 8000/60000]\nloss: 0.926029  [12000/60000]\nloss: 0.040201  [16000/60000]\nloss: 1.491799  [20000/60000]\nloss: 0.002977  [24000/60000]\nloss: 0.109589  [28000/60000]\nloss: 0.593281  [32000/60000]\nloss: 0.023370  [36000/60000]\nloss: 0.031567  [40000/60000]\nloss: 0.183430  [44000/60000]\nloss: 0.024935  [48000/60000]\nloss: 0.000807  [52000/60000]\nloss: 0.066379  [56000/60000]\nTest Error: \n Accuracy: 95.7%, Avg loss: 0.138501 \n\nEpoch 2\n-------------------------------\nloss: 0.044112  [    0/60000]\nloss: 0.116765  [ 4000/60000]\nloss: 0.095042  [ 8000/60000]\nloss: 0.001941  [12000/60000]\nloss: 0.036705  [16000/60000]\nloss: 0.023863  [20000/60000]\nloss: 0.476026  [24000/60000]\nloss: 0.001462  [28000/60000]\nloss: 0.001608  [32000/60000]\nloss: 0.001597  [36000/60000]\nloss: 0.038754  [40000/60000]\nloss: 0.007304  [44000/60000]\nloss: 0.001621  [48000/60000]\nloss: 1.779007  [52000/60000]\nloss: 0.000143  [56000/60000]\nTest Error: \n Accuracy: 96.9%, Avg loss: 0.095119 \n\nEpoch 3\n-------------------------------\nloss: 0.060063  [    0/60000]\nloss: 0.007769  [ 4000/60000]\nloss: 0.016728  [ 8000/60000]\nloss: 0.331042  [12000/60000]\nloss: 0.157323  [16000/60000]\nloss: 0.005270  [20000/60000]\nloss: 0.162094  [24000/60000]\nloss: 0.000406  [28000/60000]\nloss: 0.005161  [32000/60000]\nloss: 0.268146  [36000/60000]\nloss: 0.016454  [40000/60000]\nloss: 0.000236  [44000/60000]\nloss: 0.055423  [48000/60000]\nloss: 0.006291  [52000/60000]\nloss: 0.000155  [56000/60000]\nTest Error: \n Accuracy: 97.7%, Avg loss: 0.077697 \n\nEpoch 4\n-------------------------------\nloss: 0.012700  [    0/60000]\nloss: 0.002099  [ 4000/60000]\nloss: 0.005071  [ 8000/60000]\nloss: 0.001560  [12000/60000]\nloss: 0.001749  [16000/60000]\nloss: 0.001361  [20000/60000]\nloss: 0.004884  [24000/60000]\nloss: 0.000139  [28000/60000]\nloss: 0.000247  [32000/60000]\nloss: 0.003799  [36000/60000]\nloss: 0.041536  [40000/60000]\nloss: 0.001747  [44000/60000]\nloss: 0.001039  [48000/60000]\nloss: 0.013598  [52000/60000]\nloss: 0.012482  [56000/60000]\nTest Error: \n Accuracy: 97.8%, Avg loss: 0.078340 \n\nEpoch 5\n-------------------------------\nloss: 0.000037  [    0/60000]\nloss: 0.002917  [ 4000/60000]\nloss: 0.001830  [ 8000/60000]\nloss: 0.003051  [12000/60000]\nloss: 0.000172  [16000/60000]\nloss: 0.000025  [20000/60000]\nloss: 0.000012  [24000/60000]\nloss: 0.000078  [28000/60000]\nloss: 0.000097  [32000/60000]\nloss: 0.000528  [36000/60000]\nloss: 0.000100  [40000/60000]\nloss: 0.332108  [44000/60000]\nloss: 0.000370  [48000/60000]\nloss: 0.002161  [52000/60000]\nloss: 0.001802  [56000/60000]\nTest Error: \n Accuracy: 98.2%, Avg loss: 0.064593 \n\nDone!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-51c32633-4910-4cab-93a1-44f020a7ff94",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9b13cd88",
    "execution_start": 1624608820598,
    "execution_millis": 1424672,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1745
   },
   "source": "import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nmnist = datasets.ImageFolder('/work/mnist', transform=transforms.Compose([\n                    transforms.ToTensor(), transforms.Grayscale(1), transforms.Normalize((0.1307,), (0.3081,))]))\ntrain_dataset, test_dataset = torch.utils.data.random_split(mnist, [60000, 10000])\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 5)\n        self.fc1   = nn.Linear(1024, 128)\n        #self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(128, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        #out = F.relu(self.fc2(out))\n        out = F.log_softmax(self.fc3(out))\n        return out\nmodel = NeuralNetwork()\nprint(model)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_size = len(train_dataloader.dataset)\n    for batch, (X, y) in enumerate(train_dataloader):\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if batch % 1000 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n    test_size = len(test_dataloader.dataset)\n    num_batches = len(test_dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in test_dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= test_size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\nprint(\"Done!\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "NeuralNetwork(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n)\nEpoch 1\n-------------------------------\nloss: 2.331523  [    0/60000]\n<ipython-input-5-1919d6007d44>:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  out = F.log_softmax(self.fc3(out))\nloss: 1.377386  [ 4000/60000]\nloss: 0.017180  [ 8000/60000]\nloss: 0.017855  [12000/60000]\nloss: 0.000597  [16000/60000]\nloss: 0.151512  [20000/60000]\nloss: 0.063900  [24000/60000]\nloss: 0.015859  [28000/60000]\nloss: 0.000100  [32000/60000]\nloss: 1.071541  [36000/60000]\nloss: 0.007967  [40000/60000]\nloss: 0.000157  [44000/60000]\nloss: 0.001458  [48000/60000]\nloss: 0.002475  [52000/60000]\nloss: 0.001280  [56000/60000]\nTest Error: \n Accuracy: 98.2%, Avg loss: 0.059471 \n\nEpoch 2\n-------------------------------\nloss: 0.121991  [    0/60000]\nloss: 0.000098  [ 4000/60000]\nloss: 0.000739  [ 8000/60000]\nloss: 0.108760  [12000/60000]\nloss: 0.000101  [16000/60000]\nloss: 0.008634  [20000/60000]\nloss: 0.000245  [24000/60000]\nloss: 0.000810  [28000/60000]\nloss: 0.000379  [32000/60000]\nloss: 0.001223  [36000/60000]\nloss: 0.004260  [40000/60000]\nloss: 0.028971  [44000/60000]\nloss: 0.009085  [48000/60000]\nloss: 0.001290  [52000/60000]\nloss: 0.000682  [56000/60000]\nTest Error: \n Accuracy: 98.9%, Avg loss: 0.035636 \n\nEpoch 3\n-------------------------------\nloss: 0.000028  [    0/60000]\nloss: 0.000001  [ 4000/60000]\nloss: 0.024677  [ 8000/60000]\nloss: 0.000732  [12000/60000]\nloss: 0.000149  [16000/60000]\nloss: 0.000136  [20000/60000]\nloss: 0.050346  [24000/60000]\nloss: 0.200522  [28000/60000]\nloss: 0.000582  [32000/60000]\nloss: 0.000139  [36000/60000]\nloss: 0.000110  [40000/60000]\nloss: 0.011373  [44000/60000]\nloss: 0.019473  [48000/60000]\nloss: 0.000529  [52000/60000]\nloss: 0.001499  [56000/60000]\nTest Error: \n Accuracy: 98.8%, Avg loss: 0.039226 \n\nEpoch 4\n-------------------------------\nloss: 0.000531  [    0/60000]\nloss: 0.000113  [ 4000/60000]\nloss: 0.000803  [ 8000/60000]\nloss: 0.014575  [12000/60000]\nloss: 0.000245  [16000/60000]\nloss: 0.000001  [20000/60000]\nloss: 0.000062  [24000/60000]\nloss: 0.000004  [28000/60000]\nloss: 0.000043  [32000/60000]\nloss: 0.000468  [36000/60000]\nloss: 0.006432  [40000/60000]\nloss: 0.002041  [44000/60000]\nloss: 0.000107  [48000/60000]\nloss: 0.000024  [52000/60000]\nloss: 0.000037  [56000/60000]\nTest Error: \n Accuracy: 99.0%, Avg loss: 0.032166 \n\nEpoch 5\n-------------------------------\nloss: 0.000932  [    0/60000]\nloss: 0.000031  [ 4000/60000]\nloss: 0.002481  [ 8000/60000]\nloss: 0.000009  [12000/60000]\nloss: 0.000082  [16000/60000]\nloss: 0.000043  [20000/60000]\nloss: 0.000022  [24000/60000]\nloss: 0.000044  [28000/60000]\nloss: 0.000000  [32000/60000]\nloss: 0.000003  [36000/60000]\nloss: 0.000038  [40000/60000]\nloss: 0.002463  [44000/60000]\nloss: 0.000001  [48000/60000]\nloss: 0.000000  [52000/60000]\nloss: 0.000323  [56000/60000]\nTest Error: \n Accuracy: 99.0%, Avg loss: 0.033722 \n\nDone!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-892dd2ad-dcbe-46b4-83d9-2f00958098b7",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9386bc5c",
    "execution_start": 1624843010817,
    "execution_millis": 1326658,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1817
   },
   "source": "import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nmnist = datasets.ImageFolder('/work/mnist', transform=transforms.Compose([\n                    transforms.ToTensor(), transforms.Grayscale(1), transforms.Normalize((0.1307,), (0.3081,))]))\ntrain_dataset, test_dataset = torch.utils.data.random_split(mnist, [60000, 10000])\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nmodel = NeuralNetwork()\nprint(model)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_size = len(train_dataloader.dataset)\n    for batch, (X, y) in enumerate(train_dataloader):\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if batch % 1000 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n    test_size = len(test_dataloader.dataset)\n    num_batches = len(test_dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in test_dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= test_size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\nprint(\"Done!\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "NeuralNetwork(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout(p=0.25, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\nEpoch 1\n-------------------------------\nloss: 2.339047  [    0/60000]\n/shared-libs/python3.8/py/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nloss: 0.743810  [ 4000/60000]\nloss: 0.184150  [ 8000/60000]\nloss: 0.014598  [12000/60000]\nloss: 0.005888  [16000/60000]\nloss: 0.001075  [20000/60000]\nloss: 0.026720  [24000/60000]\nloss: 0.033342  [28000/60000]\nloss: 0.005402  [32000/60000]\nloss: 0.022702  [36000/60000]\nloss: 0.000022  [40000/60000]\nloss: 0.005843  [44000/60000]\nloss: 0.144232  [48000/60000]\nloss: 0.317386  [52000/60000]\nloss: 0.018345  [56000/60000]\nTest Error: \n Accuracy: 98.1%, Avg loss: 0.066703 \n\nEpoch 2\n-------------------------------\nloss: 0.110269  [    0/60000]\nloss: 0.000717  [ 4000/60000]\nloss: 0.004570  [ 8000/60000]\nloss: 0.000293  [12000/60000]\nloss: 0.000093  [16000/60000]\nloss: 0.000628  [20000/60000]\nloss: 0.000334  [24000/60000]\nloss: 0.000270  [28000/60000]\nloss: 0.000013  [32000/60000]\nloss: 0.000092  [36000/60000]\nloss: 0.000402  [40000/60000]\nloss: 0.006363  [44000/60000]\nloss: 0.001631  [48000/60000]\nloss: 0.000019  [52000/60000]\nloss: 0.000051  [56000/60000]\nTest Error: \n Accuracy: 98.8%, Avg loss: 0.039686 \n\nEpoch 3\n-------------------------------\nloss: 0.000014  [    0/60000]\nloss: 0.003178  [ 4000/60000]\nloss: 0.000000  [ 8000/60000]\nloss: 0.000035  [12000/60000]\nloss: 0.147903  [16000/60000]\nloss: 0.003594  [20000/60000]\nloss: 0.003230  [24000/60000]\nloss: 0.000095  [28000/60000]\nloss: 0.002397  [32000/60000]\nloss: 0.001331  [36000/60000]\nloss: 0.002101  [40000/60000]\nloss: 0.000048  [44000/60000]\nloss: 0.000046  [48000/60000]\nloss: 0.021444  [52000/60000]\nloss: 0.000129  [56000/60000]\nTest Error: \n Accuracy: 98.8%, Avg loss: 0.040541 \n\nEpoch 4\n-------------------------------\nloss: 0.000899  [    0/60000]\nloss: 0.000004  [ 4000/60000]\nloss: 0.165152  [ 8000/60000]\nloss: 0.000913  [12000/60000]\nloss: 0.000001  [16000/60000]\nloss: 0.000008  [20000/60000]\nloss: 0.000004  [24000/60000]\nloss: 0.000019  [28000/60000]\nloss: 0.000019  [32000/60000]\nloss: 0.000971  [36000/60000]\nloss: 0.000056  [40000/60000]\nloss: 0.000003  [44000/60000]\nloss: 0.000221  [48000/60000]\nloss: 0.000143  [52000/60000]\nloss: 0.000001  [56000/60000]\nTest Error: \n Accuracy: 98.9%, Avg loss: 0.036219 \n\nEpoch 5\n-------------------------------\nloss: 0.107500  [    0/60000]\nloss: 0.013926  [ 4000/60000]\nloss: 0.000000  [ 8000/60000]\nloss: 0.000006  [12000/60000]\nloss: 0.000011  [16000/60000]\nloss: 0.000394  [20000/60000]\nloss: 0.026861  [24000/60000]\nloss: 0.000000  [28000/60000]\nloss: 0.000000  [32000/60000]\nloss: 0.000040  [36000/60000]\nloss: 0.000257  [40000/60000]\nloss: 0.000002  [44000/60000]\nloss: 0.000030  [48000/60000]\nloss: 0.032546  [52000/60000]\nloss: 0.000676  [56000/60000]\nTest Error: \n Accuracy: 99.0%, Avg loss: 0.037180 \n\nDone!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=26e5d5b4-a604-4f7b-aef4-7756f8127ef0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "508ccc79-4530-45a4-8ab7-4a5cfb863ea9",
  "deepnote_execution_queue": []
 }
}